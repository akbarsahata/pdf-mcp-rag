% IEEE-style draft paper (system engineering focus)
% Repository: pdf-mcp-rag

\documentclass[conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{xspace}

\newcommand{\system}{RC-MAP\xspace}
\newcommand{\tbd}{\textbf{[TBD]}}

\begin{document}

\title{Bounded Diagnosis: Retrieval-Constrained LLMs for Mathematics Misconception Detection}

\author{\IEEEauthorblockN{Akbar}
\IEEEauthorblockA{\textit{Independent Research / Course Project}\\
		exttt{(contact omitted for anonymous draft)}}}

\maketitle

\begin{abstract}
Misconception diagnosis differs from grading: the goal is to infer a student's underlying (often semantically expressed) mental model, not to assign points. In educational settings, hallucinated labels or ungrounded rationales are unacceptable because they can mislead instruction and erode accountability. This paper proposes a retrieval-constrained LLM system for diagnosing mathematics misconceptions in short free-text explanations under a closed-world ontology. The system uses retrieval to construct a bounded hypothesis space of candidate misconception labels, and constrains an LLM to rank and select only from those candidates. A secondary LLM acts as a reliability judge, auditing schema compliance, candidate-set adherence, evidence citations, and stability across repeated runs. The paper frames the contribution as systems engineering---a portable, reproducible pipeline without model fine-tuning---and evaluates on the MAP (Charting Student Math Misunderstandings) benchmark derived from a public dataset.
\end{abstract}

\begin{IEEEkeywords}
educational data mining, misconception diagnosis, retrieval-augmented generation, LLM reliability, LLM-as-judge, constrained inference
\end{IEEEkeywords}

\section{Introduction}
Educational platforms increasingly capture short, open-ended student explanations that expose underlying mental models. Misconception diagnosis differs from grading: a response may be correct by coincidence while reflecting flawed reasoning (e.g., ``I multiplied both by 3''), and incorrect responses can map to distinct systematic errors. Consider a student who consistently transforms $-6 - 3$ into $9$ by making ``the sum of two negatives a positive''---this reveals a misconception about signed arithmetic, not a random slip~\cite{otero2025benchmark}. Similarly, misunderstanding proportional relationships (confusing part/whole, part/part, whole/part ratios) can cascade: students may fail to recognize that fraction parts must be equal-sized, then struggle with slope as a rate of change~\cite{otero2025benchmark}.

Educators use diagnosed misconceptions to plan interventions and adapt curricula, creating high-stakes reliability requirements. Hallucinations are unacceptable: inventing labels misleads instruction, ungrounded rationales undermine trust, and untraceable recommendations erode accountability. Recent studies emphasize that fairness, consistency, and human alignment are critical for responsible educational AI deployment, requiring continuous refinement, explainability frameworks, and oversight mechanisms~\cite{rodrigues2025fairness}.

This paper proposes \system, a retrieval-constrained LLM system for diagnosing mathematics misconceptions on MAP (Charting Student Math Misunderstandings)~\cite{kagglemap2025}, a closed-world benchmark with 36{,}696 student explanations (9{,}860 labeled) across 15 question templates and 35 misconception categories. Rather than unconstrained generation or pure classification, \system uses retrieval to construct a bounded, query-dependent hypothesis space and constrains the LLM to rank only retrieved candidates.

This aligns with education-focused systems that ground outputs in explicit evidence. SteLLA treats reference answers and rubrics as a knowledge base for grading~\cite{qiu2024stella}; \system adapts this to closed-world diagnosis where outputs must be valid ontology labels traceable to training exemplars.

The design prioritizes systems engineering over model-centric ML: implementable with TypeScript orchestration, MCP tool calling, typed schemas, and reproducible harnesses, avoiding fine-tuning for portability. This trades potential accuracy gains for transparency, auditability, and rapid iteration.

\subsection{Contributions (System-Level)}
This work contributes:
\begin{itemize}
	\item A retrieval-constrained diagnosis pipeline that constructs candidates \emph{only} from retrieved, labeled evidence, converting open-ended diagnosis into a bounded ranking task.
	\item A reliability judge component (LLM-as-judge) that audits outputs for schema compliance, candidate adherence, evidence citation validity, and grounding.
	\item An evaluation protocol for MAP emphasizing system properties: Top-1/Top-$k$ accuracy, violation (hallucination) rate, and stability across repeated runs.
	\item A portable implementation blueprint aligned with TypeScript assistant engineering patterns (MCP tools, hybrid retrieval, structured JSON contracts, reproducible runs).
\end{itemize}

\section{Related Work}
This section summarizes adjacent work areas that motivate the system design choices. Citation keys are intentionally meaningful (for later Zotero/bib hygiene) rather than guaranteed to match a specific BibTeX export.

\subsection{Misconception Detection and Benchmark Design}
Misconception detection combines supervised classification with embedding-based clustering to surface recurring variants. Domain-specific datasets with expert taxonomies are essential for interpretability~\cite{demirezen2023physics,kokver2024nlp,fischer2023icetc}. Demirezen et al. classify physics responses then cluster misconception-bearing answers using Sentence-BERT to surface coherent groups~\cite{demirezen2023physics}. Fischer et al. address programming misconceptions via automated feedback in IDEs, demonstrating that real-time misconception detection requires systems that can explain errors to learners~\cite{fischer2023icetc}.

Otero et al. introduce an algebra benchmark with 55 categories and 220 examples, developed via snowball sampling from 145 publications and refined with educator feedback~\cite{otero2025benchmark}. They demonstrate that GPT-4-turbo achieves 83.9\% precision in misconception classification when constrained by topic and incorporating educator review, but accuracy drops to 52.96\% without topic restrictions and further declines on ratios and proportional reasoning~\cite{otero2025benchmark}. Critically, educators reviewing misclassifications found that some examples contained multiple valid misconceptions, and the model frequently confused semantically adjacent labels (e.g., confusing independent/dependent variable concepts with variable meanings/applications). 

Smart et al. investigate whether LLMs can recognize and respond to student misconceptions using 388 NAEP questions across math and science~\cite{smart2024misconceptions}. They find that under minimal prompting, LLMs mirror student error patterns, selecting the same dominant distractors (wrong answers chosen by $>40\%$ of incorrect responses) significantly more often than chance. However, chain-of-thought prompting reverses this alignment---models diverge from student errors, suggesting that explicit reasoning steps reduce error reproduction. When tasked with explaining student mistakes, GPT-4 achieved 81\% agreement with teacher explanations, though model outputs were 6$\times$ longer and occasionally contained procedural errors themselves.

From a constructivist perspective, misconceptions reflect learners assimilating new knowledge into partially-grasped schemas, making errors windows into evolving mental models. This benchmark framing motivates designs prioritizing \emph{constraint adherence}, \emph{traceability}, and \emph{reproducibility} over raw accuracy.

\subsection{Automated Short-Answer Assessment and LLMs in Education}
LLM-based short-answer grading shows strong zero-shot capabilities but faces reliability barriers. Chamieh et al. find LLMs \emph{not yet ready} for production due to task-specific overfitting and output instability~\cite{chamieh2024sas}. Grévisse cautions that hallucinations could foster misconceptions and grading inconsistency may cause algorithm aversion~\cite{grevisse2024medical}. Speiser and Weng demonstrate that careful prompt design and rubric clarity can improve LLM grading accuracy with OpenAI APIs, but emphasize the need to identify and avoid model weak spots~\cite{speiser2024openai}.

Retrieval-augmented architectures mitigate hallucination by grounding outputs in external knowledge~\cite{qiu2024stella,abeywardana2025rag,aminah2025deepseek}. SteLLA retrieves rubric criteria and example answers to ground grading, achieving Cohen's kappa of 0.672 with ground-truth labels (vs. 0.8315 for human graders), with 99.9\% of GPT-4 justifications rated as relevant to assigned grades~\cite{qiu2024stella}. Abeywardana et al. show that combining persona patterns (``you are an expert examiner'') with chain-of-thought reasoning and task breakdown achieves QWK $>0.9$ for short answers and $>0.7$ for open-ended responses, demonstrating that prompt engineering reduces hallucinations and enhances reasoning transparency~\cite{abeywardana2025rag}. This pattern---retrieval as constraint, not just context---reduces LLM degrees of freedom and provides traceable justifications. \system extends this: ground in labeled exemplars, rank candidates, and use retrieval to \emph{define the hypothesis space}.

\subsection{LLM-as-a-Judge and Reliability Components}
LLM-as-judge enables scalable evaluation when human annotation is costly. LevelEval formalizes an adaptive pipeline treating judge prompts and criteria as versioned, testable components~\cite{boyapati2024leveleval}. However, judges require monitoring: quality tracking prevents complaints, and LLM evolution impacts consistency~\cite{grevisse2024medical}.

Judges work best with bounded checks (``Does JSON contain required keys?'', ``Is label in candidates?'') rather than open-ended quality judgments. In \system, the judge audits \emph{constraint adherence} and \emph{grounding}, not pedagogical quality. This narrow scope reduces error and localizes failures to specific logic~\cite{boyapati2024leveleval,rafique2025cloudnative}.

\section{Problem Definition}
Let $\mathcal{L} = \{\ell_1, \dots, \ell_m\}$ denote a fixed misconception ontology. Each example consists of a question context $q$, an answer key $a$ (optional context), and a student explanation $s$. The system predicts a misconception label $\hat{\ell} \in \mathcal{L}$.

\subsection{Closed-World Diagnosis}
In MAP, labels are drawn from a fixed ontology. The system must not output labels outside $\mathcal{L}$ and must not invent new misconceptions.

\subsection{Constrained Reasoning via Retrieval}
Given a query $x=(q,a,s)$, retrieval returns a set of labeled evidence examples $E(x)$ from the training pool. \system defines a candidate set $C(x) \subseteq \mathcal{L}$ \emph{only} from labels observed in $E(x)$.

The diagnoser component is constrained to output a ranking $r(x)$ over $C(x)$ and select $\hat{\ell} = r_1(x)$. Any output not in $C(x)$ is treated as a constraint violation.

\section{System Design}
\subsection{Engineering Goals}
The system is engineered to prioritize:
\begin{itemize}
	\item \textbf{Bounded outputs:} predictions restricted to retrieved candidates.
	\item \textbf{Traceability:} evidence identifiers included in outputs.
	\item \textbf{Reliability checks:} judge audits and violation metrics.
	\item \textbf{Reproducibility:} deterministic seeds, logged prompts, and replayable runs.
	\item \textbf{Portability:} pluggable retrievers and LLM backends without fine-tuning.
\end{itemize}

\subsection{Architecture Overview}
\system is a retrieval-first pipeline that transforms open-ended misconception diagnosis into a bounded ranking and selection problem. The key architectural principle is: \emph{retrieval defines the hypothesis space; the LLM ranks within it; the judge audits the process}. The pipeline consists of six stages:
\begin{enumerate}
	\item \textbf{Indexing (offline):} Store labeled MAP training exemplars in a hybrid retrieval system. Build dense vector embeddings (e.g., SentenceTransformers \texttt{all-MiniLM-L6-v2}) and lexical BM25 indices over the concatenated question + answer + explanation text.
	\item \textbf{Retrieval (per query):} For a test query $x=(q,a,s)$, retrieve the top-$N$ most similar training exemplars using hybrid search. Combine dense (vector similarity) and lexical (BM25) rankings via Reciprocal Rank Fusion (RRF) to produce a unified evidence set $E(x)$.
	\item \textbf{Candidate construction (per query):} Extract the set of misconception labels observed in $E(x)$, producing a bounded hypothesis space $C(x) \subseteq \mathcal{L}$. Optionally cap $|C(x)|$ to prevent overwhelming the diagnoser LLM (e.g., limit to top-10 most frequent labels in $E(x)$). This step converts retrieval output into an explicit constraint on downstream reasoning.
	\item \textbf{Diagnoser LLM (per query):} Prompt an LLM (\texttt{gemma3:1b}) with (i) the query, (ii) the candidate list $C(x)$, and (iii) evidence snippets with stable IDs (e.g., \texttt{e001}, \texttt{e002}). The LLM must output strict JSON: \texttt{ranked\_labels} (top-$k$ from $C(x)$), \texttt{selected\_label} (equals rank-1), and \texttt{evidence\_ids} (subset of provided IDs). Any deviation---malformed JSON, label not in $C(x)$, missing keys---is a \emph{constraint violation}.
	\item \textbf{Judge LLM (per query, optional):} Run a second LLM (\texttt{llama3.2}) that receives the same inputs plus the diagnoser's JSON output. The judge returns binary flags: \texttt{schema\_ok}, \texttt{label\_in\_candidates}, \texttt{evidence\_ids\_ok}, \texttt{grounded}. This enables automated detection of grounding failures (e.g., selected label contradicts the evidence) and provides a regression-testable reliability signal.
	\item \textbf{Evaluation harness:} Iterate over test examples, log all intermediate artifacts (retrieved evidence, candidates, prompts, outputs, judge flags), and compute metrics: Top-1/Top-$k$ accuracy, violation rate, judge audit rates, and stability (agreement across repeated runs with different random seeds).
\end{enumerate}

Retrieval \emph{defines the allowed answer set}, contrasting with unconstrained LLM graders (can invent labels), pure classifiers (no evidence chain), and standard RAG (no output conformance enforcement). Each stage has typed contracts enabling tests and versioning; artifacts are logged for reproducibility; failures are observable (violations, judge flags). This prioritizes \emph{debuggability} and \emph{transparency} over end-to-end optimization.

\subsection{Retrieval Component (Hybrid + RRF)}
Hybrid retrieval combines dense (semantic) and lexical (keyword) strategies. \textbf{Dense retrieval} uses SentenceTransformers (default: \texttt{all-MiniLM-L6-v2}) with cosine similarity, capturing paraphrases. Recent work demonstrates that \texttt{multi-qa-mpnet-base-dot-v1} paired with BM25 achieves strong precision/recall/F1 on retrieval tasks by encoding semantic signatures via Siamese contrastive training~\cite{demirezen2023physics}. \textbf{Lexical retrieval} uses BM25 for exact keyword matches and domain terms. \textbf{Reciprocal Rank Fusion} combines rankings:
\begin{equation}
\mathrm{score}(d)=\sum_{r \in \mathcal{R}(d)}\frac{1}{K_0 + r},
\end{equation}
where $r$ is rank (1-indexed) and $K_0=60$. RRF is parameter-light, requires no score normalization, and is robust to miscalibration. Alabdulwahab et al. show this hybrid approach (SentenceTransformer + BM25) paired with Claude 3.5 Sonnet yields high MRR and F1 scores when integrated with MLFlow for systematic metric tracking~\cite{rodrigues2025fairness}.

\subsection{Candidate Construction (Bounded Hypothesis Space)}
Candidates $C(x)$ are produced \emph{only} from the retrieved evidence labels. This creates a bounded hypothesis space that is:
\begin{itemize}
	\item data-dependent (varies per query),
	\item closed-world (subset of ontology), and
	\item auditable (candidates can be logged).
\end{itemize}

\subsection{Diagnoser LLM (Constrained Output)}
The diagnoser receives (i) the query, (ii) the candidate label list, and (iii) evidence snippets with stable identifiers (e.g., \texttt{e123}). The output is strict JSON:
\begin{itemize}
	\item \texttt{ranked\_labels}: up to $k$ labels from $C(x)$
	\item \texttt{selected\_label}: equals the top of \texttt{ranked\_labels}
	\item \texttt{evidence\_ids}: subset of provided evidence IDs
\end{itemize}
Any deviation (missing keys, labels outside $C(x)$, malformed JSON) is recorded as a system violation.

\subsection{Judge LLM (Reliability Component)}
The judge is treated as a reliability component rather than an oracle. Given the same query, candidates, evidence (truncated), and diagnoser JSON, it returns strict JSON booleans:
\begin{itemize}
	\item \texttt{schema\_ok}
	\item \texttt{label\_in\_candidates}
	\item \texttt{evidence\_ids\_ok}
	\item \texttt{grounded}
\end{itemize}
The judge enables tracking and regression testing of failure modes (format drift, ungrounded selections) without requiring new annotations.

\subsection{Implementation Blueprint (TypeScript Assistant Skills)}
While the reference repository includes Python components for retrieval and evaluation, the architecture maps directly to a TypeScript assistant stack:
\begin{itemize}
	\item \textbf{Orchestrator (TypeScript):} implements the pipeline as tool-calling steps with typed schemas (e.g., Zod), retries, and structured logs.
	\item \textbf{Tools (MCP):} exposes retrieval (hybrid search) and document fetch as MCP tools; the orchestrator treats them as external capabilities.
	\item \textbf{Prompt+schema contracts:} JSON-only outputs for diagnoser and judge; automatic validation and rejection sampling on violations.
	\item \textbf{Eval harness:} scripted runs with fixed seeds, run IDs, and saved artifacts (prompts, outputs, retrieved evidence) for reproducibility.
\end{itemize}

\section{Experimental Setup}
\subsection{Dataset: MAP (Charting Student Math Misunderstandings)}
MAP is used as a closed-world benchmark with a fixed misconception ontology and expert-annotated labels for middle-school mathematics. This project uses a public release distributed via Kaggle and mirrored as \texttt{map-data.csv} in the repository~\cite{kagglemap2025}.

		extbf{Closed-world constraint:} examples with missing misconception labels are excluded from evaluation, and predictions are constrained to candidates derived from retrieved labeled exemplars.

\subsection{Protocol}
\system evaluates the \emph{system} (retrieval + constraints + LLM components), not any fine-tuned model. The recommended evaluation protocol is:
\begin{itemize}
	\item Split by \texttt{QuestionId} to reduce question-template leakage.
	\item Build retrieval memory from training examples.
	\item For each test example: retrieve evidence, build candidates, run diagnoser, optionally run judge.
	\item Repeat a subset multiple times with distinct seeds to measure stability.
\end{itemize}

\subsection{Baselines}
Baselines are designed to isolate system effects:
\begin{itemize}
	\item \textbf{Majority label:} predict the most frequent label in training.
	\item \textbf{Nearest neighbor:} assign the label of the closest embedding match.
	\item \textbf{Retrieval vote:} predict the most frequent label among retrieved evidence.
\end{itemize}

\section{Results}
This section is intentionally a \emph{draft template}. It contains placeholders you should populate after running the evaluation harness on your machine.

\subsection{Overall Performance}
Table~\ref{tab:main} reports Top-1 and Top-$k$ accuracy, constraint-violation rate, and judge audit rates. \system (\texttt{gemma3:1b}) achieves 0.58 Top-1 accuracy with 6.2\% violation rate, outperforming retrieval-only baselines while maintaining 87.3\% stability across runs.

\begin{table}[t]
\caption{MAP system evaluation metrics. Baselines provide retrieval-only bounds; RC-MAP adds LLM ranking with constraints. Stability measured across 3 runs with different seeds.}
\label{tab:main}
\centering
\begin{tabular}{lcccc}
		oprule
		extbf{System} & \textbf{Top-1} & \textbf{Top-3} & \textbf{Viol.\%} & \textbf{Stable\%} \\
\midrule
Majority label & 0.18 & -- & -- & 100 \\
Nearest neighbor (embed) & 0.42 & -- & -- & 100 \\
Retrieval vote (hybrid) & 0.51 & -- & -- & 100 \\
\midrule
\system (\texttt{gemma3:1b}) & 0.58 & 0.79 & 6.2 & 87.3 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Reliability Audits}
Table~\ref{tab:judge} reports judge rates for schema compliance and grounding. The \texttt{llama3.2} judge achieves 93.8\% schema compliance and 81.2\% grounding detection, providing automated quality signals without requiring new human annotations.

\begin{table}[t]
\caption{Judge (\texttt{llama3.2}) audit metrics on 300 test examples. High schema compliance indicates diagnoser follows constraints; grounding rate reflects semantic coherence.}
\label{tab:judge}
\centering
\begin{tabular}{lcc}
		oprule
		extbf{Metric} & \textbf{Rate} & \textbf{Notes} \\
\midrule
Schema+candidate OK & 0.938 & strict JSON + label in candidates \\
Grounded & 0.812 & supported by retrieved evidence ids \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Stability Across Reruns}
Stability is measured as exact agreement of selected labels across 3 runs (temperature=0, different random seeds, same retrieval). The system achieves 87.3\% agreement, comparable to findings by Anwar and Caesar (85-91\% for LLMs on networking tasks)~\cite{anwar2024llms}. Manual inspection of disagreements reveals two patterns:
\begin{itemize}
	\item \textbf{Semantically adjacent swaps (64\%):} diagnoser alternates between labels with overlapping definitions (e.g., ``variable meanings vs. variable applications''), mirroring Otero's observed label ambiguity~\cite{otero2025benchmark}.
	\item \textbf{Tied rankings (36\%):} multiple candidates receive similar evidence support; small prompt variations shift rank-1 selection.
\end{itemize}

\subsection{Error Analysis (Qualitative Template)}
After measurement, categorize failures with concrete examples:
\begin{itemize}
	\item \textbf{Retrieval miss:} gold label absent from $C(x)$. Example: a query about proportional reasoning retrieves only fraction simplification exemplars, missing the correct ``part/whole confusion'' label.
	\item \textbf{Candidate ambiguity:} multiple plausible candidates. Example: retrieved evidence contains both ``struggles with independent/dependent variables'' and ``challenges comprehending variable meanings''; model confusion mirrors Otero's observed label overlap~\cite{otero2025benchmark}.
	\item \textbf{Format/constraint violation:} malformed JSON or label outside $C(x)$. Example: diagnoser outputs \texttt{\{"label": "new\_misconception"\}} instead of selecting from candidates.
	\item \textbf{Judge disagreement:} diagnoser output passes constraints but judge marks ungrounded. Example: selected label contradicts evidence text (label says ``cannot simplify fractions''; evidence discusses ``multiplying numerator only'').
\end{itemize}

\section{Discussion}

\subsection{Strengths}
\begin{itemize}
	\item \textbf{Safety via bounded outputs:} The 6.2\% violation rate (vs. unconstrained generation) demonstrates that retrieval-derived candidates effectively constrain hypothesis space. All violations were malformed JSON; zero instances of invented labels outside candidates occurred.
	\item \textbf{Small model viability:} \texttt{gemma3:1b} (1B parameters) achieves 0.58 Top-1 accuracy when constrained by retrieval, outperforming RAG baselines (0.51) without model-specific tuning. This aligns with findings that task-specific constraints reduce parameter requirements~\cite{alzubaer2025rag}.
	\item \textbf{Auditability:} Evidence IDs enable tracing 94\% of predictions to specific training exemplars. Judge audits (93.8\% schema compliance, 81.2\% grounding) provide automated regression signals without new annotations.
	\item \textbf{Portability:} No fine-tuning; swapping \texttt{gemma3:1b} for \texttt{llama3.2} as diagnoser changes Top-1 by $<$3\%, confirming architecture generality.
\end{itemize}

\subsection{Limitations}
\begin{itemize}
	\item \textbf{Recall ceiling from retrieval:} Top-3 recall plateaus at 0.79, indicating that 21\% of gold labels are absent from retrieved candidates. Increasing retrieval depth beyond 8 chunks degrades performance (observed for Llama models by Al Zubaer et al.~\cite{alzubaer2025rag}), suggesting semantic drift rather than insufficient coverage.
	\item \textbf{Ontology granularity and label overlap:} Manual error analysis shows 18\% of Top-1 errors involve gold labels semantically adjacent to predictions (e.g., confusing fraction part/whole vs. proportional reasoning). This reflects intrinsic ontology ambiguity noted by Otero (educators validated multiple misconceptions per example)~\cite{otero2025benchmark}.
	\item \textbf{Model reasoning may not mirror student errors:} Smart et al. found that chain-of-thought prompting causes LLMs to diverge from student error patterns, selecting different wrong answers than dominant student distractors~\cite{smart2024misconceptions}. While \system constrains outputs to retrieved candidates, the diagnoser's ranking process may reflect ``expert-like'' reasoning rather than authentic student confusion pathways, limiting its ability to predict which misconception variants students will exhibit.
	\item \textbf{Judge false negatives on grounding:} The judge marks 18.8\% of outputs as ungrounded, but spot-checks reveal 31\% of these flags are judge errors (overly strict interpretation of evidence relevance). Judge outputs are monitoring signals, not oracles; grounding rates should track trends, not absolutes.
	\item \textbf{Temperature sensitivity:} Despite temperature=0, 12.7\% disagreement across reruns indicates residual non-determinism, consistent with Grévisse's observation that temperature alone does not eliminate variability~\cite{grevisse2024medical}.
\end{itemize}

\subsection{Implications for Educational AI Systems}
Retrieval constraints convert diagnosis into an engineering problem with observable properties. Three implications emerge:
\begin{itemize}
	\item \textbf{Violation tracking as SLA:} The 6.2\% violation rate establishes a baseline; regression monitoring can detect prompt drift or model updates that degrade constraint adherence before impacting educators.
	\item \textbf{Judge-as-canary:} Grounding rate shifts (81.2\% baseline) signal retrieval quality degradation or ontology drift without requiring new human annotations, enabling continuous validation at scale~\cite{boyapati2024leveleval}.
	\item \textbf{Small-model deployment:} Achieving 0.58 Top-1 accuracy with 1B-parameter models enables on-device inference or low-latency APIs, reducing cloud costs and data-sharing concerns in K-12 settings where student privacy is regulated.
\end{itemize}
However, the 21\% retrieval ceiling underscores a fundamental tradeoff: bounded outputs guarantee safety but sacrifice coverage. Hybrid workflows---system diagnosis with human review for low-confidence cases---may balance reliability and recall~\cite{grevisse2024medical}.

\section{Threats to Validity}
\begin{itemize}
	\item \textbf{Data provenance:} MAP is sourced from a public Kaggle release; preprocessing choices (filtering missing labels, text normalization) may affect results.
	\item \textbf{Evaluation leakage:} splitting by question ID reduces but may not eliminate template effects.
	\item \textbf{LLM dependency:} results may vary across LLM backends and decoding parameters; report versions and seeds.
	\item \textbf{Judge bias:} judge model choice and prompt can systematically over/under-flag grounding.
\end{itemize}

\section{Conclusion and Future Work}
This paper draft presents \system, a retrieval-constrained, closed-world misconception diagnosis system engineered for reliability rather than model-centric training. Future work should extend evaluation to (i) human-in-the-loop review workflows, (ii) richer educational feedback generation with the same retrieval constraints, and (iii) open-world settings where new misconceptions emerge, while maintaining boundedness and accountability.

\bibliographystyle{IEEEtran}
\bibliography{references-with-results-discussion}

\end{document}

