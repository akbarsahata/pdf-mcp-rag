% IEEE-style draft paper (system engineering focus)
% Repository: pdf-mcp-rag

\documentclass[conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{xspace}

\newcommand{\system}{RC-MAP\xspace}
\newcommand{\tbd}{\textbf{[TBD]}}

\begin{document}

\title{Retrieval-Constrained Misconception Diagnosis on MAP:\\A Systems Approach for Reliable Educational LLMs}

\author{\IEEEauthorblockN{Akbar}
\IEEEauthorblockA{\textit{Independent Research / Course Project}\\
		exttt{(contact omitted for anonymous draft)}}}

\maketitle

\begin{abstract}
Misconception diagnosis differs from grading: the goal is to infer a student's underlying (often semantically expressed) mental model, not to assign points. In educational settings, hallucinated labels or ungrounded rationales are unacceptable because they can mislead instruction and erode accountability. This paper proposes a retrieval-constrained LLM system for diagnosing mathematics misconceptions in short free-text explanations under a closed-world ontology. The system uses retrieval to construct a bounded hypothesis space of candidate misconception labels, and constrains an LLM to rank and select only from those candidates. A secondary LLM acts as a reliability judge, auditing schema compliance, candidate-set adherence, evidence citations, and stability across repeated runs. The paper frames the contribution as systems engineering---a portable, reproducible pipeline without model fine-tuning---and evaluates on the MAP (Charting Student Math Misunderstandings) benchmark derived from a public dataset.
\end{abstract}

\begin{IEEEkeywords}
educational data mining, misconception diagnosis, retrieval-augmented generation, LLM reliability, LLM-as-judge, constrained inference
\end{IEEEkeywords}

\section{Introduction}
Educational platforms increasingly capture short, open-ended student explanations that expose underlying mental models. Misconception diagnosis differs from grading: a response may be correct by coincidence while reflecting flawed reasoning (e.g., ``I multiplied both by 3''), and incorrect responses can map to distinct systematic errors (e.g., ``fractions cannot be simplified'' vs.\ ``only subtract the same number from both parts'').

Educators use diagnosed misconceptions to plan interventions and adapt curricula, creating high-stakes reliability requirements. Hallucinations are unacceptable: inventing labels misleads instruction, ungrounded rationales undermine trust, and untraceable recommendations erode accountability. Recent studies emphasize that fairness, consistency, and human alignment are critical for responsible educational AI deployment, requiring continuous refinement, explainability frameworks, and oversight mechanisms~\cite{rodrigues2025fairness}.

This paper proposes \system, a retrieval-constrained LLM system for diagnosing mathematics misconceptions on MAP (Charting Student Math Misunderstandings)~\cite{kagglemap2025}, a closed-world benchmark with 36{,}696 student explanations (9{,}860 labeled) across 15 question templates and 35 misconception categories. Rather than unconstrained generation or pure classification, \system uses retrieval to construct a bounded, query-dependent hypothesis space and constrains the LLM to rank only retrieved candidates.

This aligns with education-focused systems that ground outputs in explicit evidence. SteLLA treats reference answers and rubrics as a knowledge base for grading~\cite{qiu2024stella}; \system adapts this to closed-world diagnosis where outputs must be valid ontology labels traceable to training exemplars.

The design prioritizes systems engineering over model-centric ML: implementable with TypeScript orchestration, MCP tool calling, typed schemas, and reproducible harnesses, avoiding fine-tuning for portability. This trades potential accuracy gains for transparency, auditability, and rapid iteration.

\subsection{Contributions (System-Level)}
This work contributes:
\begin{itemize}
	\item A retrieval-constrained diagnosis pipeline that constructs candidates \emph{only} from retrieved, labeled evidence, converting open-ended diagnosis into a bounded ranking task.
	\item A reliability judge component (LLM-as-judge) that audits outputs for schema compliance, candidate adherence, evidence citation validity, and grounding.
	\item An evaluation protocol for MAP emphasizing system properties: Top-1/Top-$k$ accuracy, violation (hallucination) rate, and stability across repeated runs.
	\item A portable implementation blueprint aligned with TypeScript assistant engineering patterns (MCP tools, hybrid retrieval, structured JSON contracts, reproducible runs).
\end{itemize}

\section{Related Work}
This section summarizes adjacent work areas that motivate the system design choices. Citation keys are intentionally meaningful (for later Zotero/bib hygiene) rather than guaranteed to match a specific BibTeX export.

\subsection{Misconception Detection and Benchmark Design}
Misconception detection combines supervised classification with embedding-based clustering to surface recurring variants. Domain-specific datasets with expert taxonomies are essential for interpretability~\cite{demirezen2023physics,kokver2024nlp,fischer2023icetc}. Demirezen et al. classify physics responses then cluster misconception-bearing answers using Sentence-BERT to surface coherent groups~\cite{demirezen2023physics}. Fischer et al. address programming misconceptions via automated feedback in IDEs, demonstrating that real-time misconception detection requires systems that can explain errors to learners~\cite{fischer2023icetc}.

Otero et al. introduce an algebra benchmark with 55 categories and 220 examples, developed via snowball sampling from 145 publications and refined with educator feedback~\cite{otero2025benchmark}. From a constructivist perspective, misconceptions reflect learners assimilating new knowledge into partially-grasped schemas, making errors windows into evolving mental models. Effective remediation requires engaging with mathematical details, yet real-time measurement remains challenging for novice teachers. This benchmark framing---fixed ontology, reproducible splits, learning theory grounding---motivates designs prioritizing \emph{constraint adherence}, \emph{traceability}, and \emph{reproducibility} over raw accuracy.

\subsection{Automated Short-Answer Assessment and LLMs in Education}
LLM-based short-answer grading shows strong zero-shot capabilities but faces reliability barriers. Chamieh et al. find LLMs \emph{not yet ready} for production due to task-specific overfitting and output instability~\cite{chamieh2024sas}. Gr√©visse cautions that hallucinations could foster misconceptions and grading inconsistency may cause algorithm aversion~\cite{grevisse2024medical}. Speiser and Weng demonstrate that careful prompt design and rubric clarity can improve LLM grading accuracy with OpenAI APIs, but emphasize the need to identify and avoid model weak spots~\cite{speiser2024openai}.

Retrieval-augmented architectures mitigate hallucination by grounding outputs in external knowledge~\cite{qiu2024stella,abeywardana2025rag,aminah2025deepseek}. SteLLA retrieves rubric criteria and example answers to ground grading~\cite{qiu2024stella}; Abeywardana et al. show RAG and prompt engineering improve consistency~\cite{abeywardana2025rag}; Aminah et al. integrate RAG with reasoning-focused LLMs for complex texts~\cite{aminah2025deepseek}. This pattern---retrieval as constraint, not just context---reduces LLM degrees of freedom and provides traceable justifications. \system extends this: ground in labeled exemplars, rank candidates, and use retrieval to \emph{define the hypothesis space}.

\subsection{LLM-as-a-Judge and Reliability Components}
LLM-as-judge enables scalable evaluation when human annotation is costly. LevelEval formalizes an adaptive pipeline treating judge prompts and criteria as versioned, testable components~\cite{boyapati2024leveleval}. However, judges require monitoring: quality tracking prevents complaints, and LLM evolution impacts consistency~\cite{grevisse2024medical}.

Judges work best with bounded checks (``Does JSON contain required keys?'', ``Is label in candidates?'') rather than open-ended quality judgments. In \system, the judge audits \emph{constraint adherence} and \emph{grounding}, not pedagogical quality. This narrow scope reduces error and localizes failures to specific logic~\cite{boyapati2024leveleval,rafique2025cloudnative}.

\section{Problem Definition}
Let $\mathcal{L} = \{\ell_1, \dots, \ell_m\}$ denote a fixed misconception ontology. Each example consists of a question context $q$, an answer key $a$ (optional context), and a student explanation $s$. The system predicts a misconception label $\hat{\ell} \in \mathcal{L}$.

\subsection{Closed-World Diagnosis}
In MAP, labels are drawn from a fixed ontology. The system must not output labels outside $\mathcal{L}$ and must not invent new misconceptions.

\subsection{Constrained Reasoning via Retrieval}
Given a query $x=(q,a,s)$, retrieval returns a set of labeled evidence examples $E(x)$ from the training pool. \system defines a candidate set $C(x) \subseteq \mathcal{L}$ \emph{only} from labels observed in $E(x)$.

The diagnoser component is constrained to output a ranking $r(x)$ over $C(x)$ and select $\hat{\ell} = r_1(x)$. Any output not in $C(x)$ is treated as a constraint violation.

\section{System Design}
\subsection{Engineering Goals}
The system is engineered to prioritize:
\begin{itemize}
	\item \textbf{Bounded outputs:} predictions restricted to retrieved candidates.
	\item \textbf{Traceability:} evidence identifiers included in outputs.
	\item \textbf{Reliability checks:} judge audits and violation metrics.
	\item \textbf{Reproducibility:} deterministic seeds, logged prompts, and replayable runs.
	\item \textbf{Portability:} pluggable retrievers and LLM backends without fine-tuning.
\end{itemize}

\subsection{Architecture Overview}
\system is a retrieval-first pipeline that transforms open-ended misconception diagnosis into a bounded ranking and selection problem. The key architectural principle is: \emph{retrieval defines the hypothesis space; the LLM ranks within it; the judge audits the process}. The pipeline consists of six stages:
\begin{enumerate}
	\item \textbf{Indexing (offline):} Store labeled MAP training exemplars in a hybrid retrieval system. Build dense vector embeddings (e.g., SentenceTransformers \texttt{all-MiniLM-L6-v2}) and lexical BM25 indices over the concatenated question + answer + explanation text.
	\item \textbf{Retrieval (per query):} For a test query $x=(q,a,s)$, retrieve the top-$N$ most similar training exemplars using hybrid search. Combine dense (vector similarity) and lexical (BM25) rankings via Reciprocal Rank Fusion (RRF) to produce a unified evidence set $E(x)$.
	\item \textbf{Candidate construction (per query):} Extract the set of misconception labels observed in $E(x)$, producing a bounded hypothesis space $C(x) \subseteq \mathcal{L}$. Optionally cap $|C(x)|$ to prevent overwhelming the diagnoser LLM (e.g., limit to top-10 most frequent labels in $E(x)$). This step converts retrieval output into an explicit constraint on downstream reasoning.
	\item \textbf{Diagnoser LLM (per query):} Prompt an LLM with (i) the query, (ii) the candidate list $C(x)$, and (iii) evidence snippets with stable IDs (e.g., \texttt{e001}, \texttt{e002}). The LLM must output strict JSON: \texttt{ranked\_labels} (top-$k$ from $C(x)$), \texttt{selected\_label} (equals rank-1), and \texttt{evidence\_ids} (subset of provided IDs). Any deviation---malformed JSON, label not in $C(x)$, missing keys---is a \emph{constraint violation}.
	\item \textbf{Judge LLM (per query, optional):} Run a second LLM that receives the same inputs plus the diagnoser's JSON output. The judge returns binary flags: \texttt{schema\_ok}, \texttt{label\_in\_candidates}, \texttt{evidence\_ids\_ok}, \texttt{grounded}. This enables automated detection of grounding failures (e.g., selected label contradicts the evidence) and provides a regression-testable reliability signal.
	\item \textbf{Evaluation harness:} Iterate over test examples, log all intermediate artifacts (retrieved evidence, candidates, prompts, outputs, judge flags), and compute metrics: Top-1/Top-$k$ accuracy, violation rate, judge audit rates, and stability (agreement across repeated runs with different random seeds).
\end{enumerate}

Retrieval \emph{defines the allowed answer set}, contrasting with unconstrained LLM graders (can invent labels), pure classifiers (no evidence chain), and standard RAG (no output conformance enforcement). Each stage has typed contracts enabling tests and versioning; artifacts are logged for reproducibility; failures are observable (violations, judge flags). This prioritizes \emph{debuggability} and \emph{transparency} over end-to-end optimization.

\subsection{Retrieval Component (Hybrid + RRF)}
Hybrid retrieval combines dense (semantic) and lexical (keyword) strategies. \textbf{Dense retrieval} uses SentenceTransformers \texttt{all-MiniLM-L6-v2} embeddings with cosine similarity, capturing paraphrases (e.g., ``multiplied both sides'' vs.\ ``scaled numerator and denominator''). \textbf{Lexical retrieval} uses BM25 for exact keyword matches and domain terms. \textbf{Reciprocal Rank Fusion} combines rankings:
\begin{equation}
\mathrm{score}(d)=\sum_{r \in \mathcal{R}(d)}\frac{1}{K_0 + r},
\end{equation}
where $r$ is rank (1-indexed) and $K_0=60$. RRF is parameter-light, requires no score normalization, and is robust to miscalibration.

\subsection{Candidate Construction (Bounded Hypothesis Space)}
Candidates $C(x)$ are produced \emph{only} from the retrieved evidence labels. This creates a bounded hypothesis space that is:
\begin{itemize}
	\item data-dependent (varies per query),
	\item closed-world (subset of ontology), and
	\item auditable (candidates can be logged).
\end{itemize}

\subsection{Diagnoser LLM (Constrained Output)}
The diagnoser receives (i) the query, (ii) the candidate label list, and (iii) evidence snippets with stable identifiers (e.g., \texttt{e123}). The output is strict JSON:
\begin{itemize}
	\item \texttt{ranked\_labels}: up to $k$ labels from $C(x)$
	\item \texttt{selected\_label}: equals the top of \texttt{ranked\_labels}
	\item \texttt{evidence\_ids}: subset of provided evidence IDs
\end{itemize}
Any deviation (missing keys, labels outside $C(x)$, malformed JSON) is recorded as a system violation.

\subsection{Judge LLM (Reliability Component)}
The judge is treated as a reliability component rather than an oracle. Given the same query, candidates, evidence (truncated), and diagnoser JSON, it returns strict JSON booleans:
\begin{itemize}
	\item \texttt{schema\_ok}
	\item \texttt{label\_in\_candidates}
	\item \texttt{evidence\_ids\_ok}
	\item \texttt{grounded}
\end{itemize}
The judge enables tracking and regression testing of failure modes (format drift, ungrounded selections) without requiring new annotations.

\subsection{Implementation Blueprint (TypeScript Assistant Skills)}
While the reference repository includes Python components for retrieval and evaluation, the architecture maps directly to a TypeScript assistant stack:
\begin{itemize}
	\item \textbf{Orchestrator (TypeScript):} implements the pipeline as tool-calling steps with typed schemas (e.g., Zod), retries, and structured logs.
	\item \textbf{Tools (MCP):} exposes retrieval (hybrid search) and document fetch as MCP tools; the orchestrator treats them as external capabilities.
	\item \textbf{Prompt+schema contracts:} JSON-only outputs for diagnoser and judge; automatic validation and rejection sampling on violations.
	\item \textbf{Eval harness:} scripted runs with fixed seeds, run IDs, and saved artifacts (prompts, outputs, retrieved evidence) for reproducibility.
\end{itemize}

\section{Experimental Setup}
\subsection{Dataset: MAP (Charting Student Math Misunderstandings)}
MAP is used as a closed-world benchmark with a fixed misconception ontology and expert-annotated labels for middle-school mathematics. This project uses a public release distributed via Kaggle and mirrored as \texttt{map-data.csv} in the repository~\cite{kagglemap2025}.

		extbf{Closed-world constraint:} examples with missing misconception labels are excluded from evaluation, and predictions are constrained to candidates derived from retrieved labeled exemplars.

\subsection{Protocol}
\system evaluates the \emph{system} (retrieval + constraints + LLM components), not any fine-tuned model. The recommended evaluation protocol is:
\begin{itemize}
	\item Split by \texttt{QuestionId} to reduce question-template leakage.
	\item Build retrieval memory from training examples.
	\item For each test example: retrieve evidence, build candidates, run diagnoser, optionally run judge.
	\item Repeat a subset multiple times with distinct seeds to measure stability.
\end{itemize}

\subsection{Baselines}
Baselines are designed to isolate system effects:
\begin{itemize}
	\item \textbf{Majority label:} predict the most frequent label in training.
	\item \textbf{Nearest neighbor:} assign the label of the closest embedding match.
	\item \textbf{Retrieval vote:} predict the most frequent label among retrieved evidence.
\end{itemize}

\section{Results}
This section is intentionally a \emph{draft template}. It contains placeholders you should populate after running the evaluation harness on your machine.

\subsection{Overall Performance (TBD)}
Table~\ref{tab:main} reports Top-1 and Top-$k$ accuracy, constraint-violation rate, and judge audit rates.

\begin{table}[t]
\caption{MAP system evaluation metrics (placeholders). Replace \tbd{} with measured values.}
\label{tab:main}
\centering
\begin{tabular}{lcccc}
		oprule
		extbf{System} & \textbf{Top-1} & \textbf{Top-3} & \textbf{Viol.\%} & \textbf{Stable\%} \\
\midrule
Majority label & \tbd & -- & -- & -- \\
Nearest neighbor (embed) & \tbd & -- & -- & -- \\
Retrieval vote (hybrid) & \tbd & -- & -- & -- \\
\midrule
\system (retrieval-constrained) & \tbd & \tbd & \tbd & \tbd \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Reliability Audits (TBD)}
Table~\ref{tab:judge} reports judge rates for schema compliance and grounding. These are system reliability indicators rather than accuracy metrics.

\begin{table}[t]
\caption{Judge audit metrics (placeholders).}
\label{tab:judge}
\centering
\begin{tabular}{lcc}
		oprule
		extbf{Metric} & \textbf{Rate} & \textbf{Notes} \\
\midrule
Schema+candidate OK & \tbd & strict JSON + label in candidates \\
Grounded & \tbd & supported by retrieved evidence ids \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Stability Across Reruns (TBD)}
Stability is measured as exact agreement of selected labels across $n$ repeated runs (different random seeds, same retrieval).
\begin{itemize}
	\item Agreement rate: \tbd
	\item Common instability pattern: \tbd{} (e.g., swapping among semantically adjacent candidates)
\end{itemize}

\subsection{Error Analysis (Qualitative Template)}
After measurement, include a small, anonymous sample of failures and categorize them:
\begin{itemize}
	\item \textbf{Retrieval miss:} gold label absent from $C(x)$ due to evidence set.
	\item \textbf{Candidate ambiguity:} multiple plausible candidates retrieved.
	\item \textbf{Format/constraint violation:} malformed JSON or label outside candidates.
	\item \textbf{Judge disagreement:} diagnoser output passes constraints but judge marks ungrounded.
\end{itemize}

\section{Discussion}
This section contains pseudo-content scaffolding; update after results are populated.

\subsection{Strengths (Expected)}
\begin{itemize}
	\item \textbf{Safety via bounded outputs:} retrieval-derived candidates reduce open-ended hallucinated labels.
	\item \textbf{Auditability:} evidence IDs enable tracing decisions back to training exemplars.
	\item \textbf{Portability:} no fine-tuning; LLM and retriever components are swappable.
\end{itemize}

\subsection{Limitations (Expected)}
\begin{itemize}
	\item \textbf{Recall limited by retrieval:} if the correct misconception is not retrieved, the system cannot predict it.
	\item \textbf{Ontology granularity:} fixed labels can collapse meaningful subtypes of reasoning.
	\item \textbf{Judge fallibility:} judge outputs should be treated as signals, not ground truth.
\end{itemize}

\subsection{Implications for Educational AI Systems}
In settings where educators require accountability, retrieval-constrained diagnosis offers a pragmatic alternative to unconstrained LLM graders: it prioritizes bounded outputs, structured artifacts, and measurable reliability signals (violations, stability). These are engineering properties that can be monitored over time and across deployments.

\section{Threats to Validity}
\begin{itemize}
	\item \textbf{Data provenance:} MAP is sourced from a public Kaggle release; preprocessing choices (filtering missing labels, text normalization) may affect results.
	\item \textbf{Evaluation leakage:} splitting by question ID reduces but may not eliminate template effects.
	\item \textbf{LLM dependency:} results may vary across LLM backends and decoding parameters; report versions and seeds.
	\item \textbf{Judge bias:} judge model choice and prompt can systematically over/under-flag grounding.
\end{itemize}

\section{Conclusion and Future Work}
This paper draft presents \system, a retrieval-constrained, closed-world misconception diagnosis system engineered for reliability rather than model-centric training. Future work should extend evaluation to (i) human-in-the-loop review workflows, (ii) richer educational feedback generation with the same retrieval constraints, and (iii) open-world settings where new misconceptions emerge, while maintaining boundedness and accountability.

\bibliographystyle{IEEEtran}
\bibliography{references}

\end{document}

